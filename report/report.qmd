---
title: "ECON832 Final: Mini Paper"
author: "Daniel Sánchez Pazmiño"
date: "2024-04-24"
date-format: "MMMM YYYY"
output-file: "SANCHEZ_Daniel_final_minipaper.pdf"
format: pdf # Change to html if LaTeX not installed or causing issues
execute:
    echo: false
    warning: false
    message: false
fig-align: center
fig-pos: h
geometry: 
    - margin = 1in
fig-cap-location: top
tbl-cap-location: top
number-sections: true
fontsize: 11pt
bibliography: references.bib
header-includes:
    - \usepackage{float}
    - \usepackage{amsmath, amsfonts, amssymb, amsthm}
---

```{julia}
#| include: false
#=
To compile this Quarto notebook, run the following in the terminal:
using Pkg
Pkg.add("IJulia")
using IJulia
notebook()
Needs tinytex or a full LaTeX distribution for PDF output, change format to html or docx if not available
=#

cd("../.")

using Pkg
Pkg.activate("final_env")

using TidierFiles, PrettyTables
```

# Overview 

The data comes from the CPC18 [@plonsky_etal17], which consists of experiments involving decision makers choosing between two lotteries. I focus on data exclusive to CPC18 (Experiment 1), which added attention features to the CPC15 data [@erev_etal17]. This experiment involved sixty choice problems for 240 participants. I use the calibration data set to train my deep feedforward neural network and the **individual track data** (Track II) to test the model. All replicating code and files are available on the project's GitHub repository. 

<!-- Two sets of problems were implemented, where participants faced one set of thirty problems. Each problem was faced for 25 trials, where the first five trials did not provide feedback about forgone and obtained payoffs. A trial involved choosing between two options, where each option (A or B) involved an underlying lottery with a probability distribution [@plonsky_etal18]. I use the calibration data set to train the DFNNs and the **individual track data** (Track II) to test the model. -->

# Methodology

## Data preparation

The calibration dataset was filtered to only include Experiment 1 data (sets 5 and 6). The outcome variable was the binary indicator of the choice of B over A. Since the variable was already dichotomous, no transformation was needed. I include demographic variables in the baseline models: location, gender and age. I dichotomized categorical variables with Technion and male as the reference levels. 

Risk features involve the number of lottery outcomes, the expected values, the low payoffs, and probabilities to draw from the lotteries and lot distribution shapes. The latter defines the distribution of the lotteries, which can be binomial around the expected value, right skewed or left skewed. They were dichotomized in the same way as the location and gender variables. 

Other relevant variables involve whether there is ambiguity in the probabilities or if there is a correlation between the payoffs. Further, it is important to include trial numbers, potential and obtained payoffs obtained, and forgone payoffs. Some of these variables are categorical, but they are already dichotomized, so I didn't transform them.

Regarding attention features, I include the number of time block, reaction time, feedback binary outcome, the onscreen button and the trial number within a game. 

The processing of the calibration dataset was done using `TidierData.jl` and `DataFrames.jl`. I performed a test-train split with 80% of the data used for training and 20% for testing. I further tested with the individual track data @plonsky_etal_comp_ind. 

## Feature engineering

As mentioned, all categorical variables for both the risk and attention features were dichotomized. This is equivalent to **one-hot encoding**. The continuous variables were **standardized** to have $\mu = 0$ and $\sigma = 1$. The data was processed from its rectangular form (every row per decision) to a format where each row represented an attribute, and every column represented a decision.

## Model configuration

The model is a deep feedforward network neural network (DFNN) with `Flux.jl`. I use a ReLU activation function for the input and hidden layers and a sigmoid activation function for the output layer. I use MSE as the loss function and the gradient descent optimizer to minimize the loss function. The learning rate was set to 0.7, and I train the model for 100 epochs using a `for` loop. I use a confusion matrix and an accuracy rate (percent correctly predicted) to evalute the model's fit, using a 0.5 probability treshold to determine predicted classes.

# Feature analysis

## Risk features

The correlation between the number of outcomes and the choice for the B lottery is 0.09, which is statistically significant at the 1% level. Further, the correlation between the low payoff of the lotteries and the choice for the B lottery is 0.11, which is statistically significant at the 1% level. This might be a result of the risk aversion of the participants, which is a common feature in decision-making and point towards the importance of considering risk-aversion in the model rather than risk ne
utrality. The correlation between lotteries shows a relationship with the choice of B, with negative correlation of -0.14, which is statistically significant at the 1% level. 

:::{#fig-features layout-ncol=2}

![By shape of lottery distribution](../figures/barplot_by_lotshape.png){#fig-shapes height=7cm}

![By payoff feedback](../figures/barplot_by_feedback.png){#fig-payoff height=7cm}

Choices of lottery by attention and risk features
:::

In terms of the shape of the distribution, I identify that a symmetric distribution greatly favours the choice of B, as seen in @fig-shapes. Further, I also identify that the payoff that the payoff that the participant received shows a statistically significant correlation with the choice of B. I choose to include all 12 variables which jointly define the shape of the distribution in the model, as well as the payoff that the participant received plus the potential payoffs of A and B. 

## Attention features

I find that the forgone payoff has a statistically significant correlation with the choice of B, with a correlation of -0.016. While reaction time does not show a significant correlation with the choice of B, I include it along time blocks in the interest that its inclusion might point towards the importance of attention, and particularly fatigue, in the decision-making process. Giving feedback to the participant also shows a notable correlation with the choice of B, as shown in @fig-payoff. Finally, a button placed toward the left marginally favours choice of A, with a correlation of -0.002 with the choice of B.

# Results and discussion

@tbl-confusion-baseline-dfnn-train and @tbl-confusion-baseline-dfnn-comp display confusiones matrices for the baseline model with risk features only, for both the training data and the competition data. The accuracy rate for the training data is 85.62%, while for the competition data it is 89.92%. This suggests that the model is not overfitting, as the accuracy rate is similar for both datasets. The loss value (MSE) for the training data is 0.11, while for the competition data it is 0.11 too. 

:::{#tbl-panel-dfnn layout-ncol=2}
```{julia}
#| label: tbl-confusion-baseline-dfnn-train
#| output: asis
#| tbl-cap: Training data

confusion_baseline_dfnn =
    read_csv("data/output/confusion_matrix_train_baseline_dfnn.csv")

header = ["A", "B"]

row_names = ["A", "B"]

pretty_table(confusion_baseline_dfnn, alignment = [:l, :r], backend = Val(:markdown), header = header, row_labels = row_names)

```
```{julia}
#| label: tbl-confusion-baseline-dfnn-comp
#| output: asis
#| tbl-cap: Competition data
#| tbl-column: page

confusion_baseline_dfnn =
    read_csv("data/output/confusion_matrix_competition_baseline_dfnn.csv")

header = ["A", "B"]

row_names = ["A", "B"]

pretty_table(confusion_baseline_dfnn, alignment = [:l, :r], backend = Val(:markdown), header = header, row_labels = row_names)
```
Confusion matrices for the baseline DFNN model
:::

The model which considers both risk and attention features has an accuracy rate of 89.92% for the competition data, which is higher than the baseline model. The loss value for the competition data is 0.11, which is the same as the baseline model. This suggests that the inclusion of attention features does not improve the model's fit.

:::{#tbl-panel-dfnn-attention layout-ncol=2}

```{julia}
#| label: tbl-confusion-attention-dfnn-train
#| output: asis
#| tbl-cap: Train
#| tbl-column: page
#| tbl-pos: h

confusion_attention_dfnn =
    read_csv("data/output/confusion_matrix_train_attention_dfnn.csv")

header = ["A", "B"]

row_names = ["A", "B"]

pretty_table(confusion_attention_dfnn, alignment = [:l, :r], backend = Val(:markdown), header = header, row_labels = row_names)
```

```{julia}
#| label: tbl-confusion-attention-dfnn-comp
#| output: asis
#| tbl-cap: Competition data
#| tbl-column: page
#| tbl-pos: h

confusion_attention_dfnn =
    read_csv("data/output/confusion_matrix_competition_attention_dfnn.csv")

header = ["A", "B"]

row_names = ["A", "B"]

pretty_table(confusion_attention_dfnn, alignment = [:l, :r], backend = Val(:markdown), header = header, row_labels = row_names)
```

Confusion matrices for the DFNN model with attention features
:::


# References {.unnumbered}

:::{.refs}

:::